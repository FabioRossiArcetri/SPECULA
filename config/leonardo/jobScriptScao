#!/bin/bash


#SBATCH --job-name=specula                    # Descriptive job name
#SBATCH --time=00:10:00                       # Maximum wall time (hh:mm:ss)
#SBATCH --nodes=2                             # Number of nodes to use
#SBATCH --ntasks-per-node=4                   # Number of MPI tasks per node (e.g., 1 per GPU)
#SBATCH --cpus-per-task=4                     # Number of CPU cores per task (adjust as needed)
#SBATCH --gres=gpu:4                          # Number of GPUs per node (adjust to match hardware)
#SBATCH --partition=boost_usr_prod            # GPU-enabled partition
#SBATCH --qos=boost_qos_dbg                   # Quality of Service
#SBATCH --output=speculaJobScao.out         # File for standard output
#SBATCH --error=speculaJobScao.err          # File for standard error
#SBATCH --account=try25_rossi                 # Project account number

# Load necessary modules (adjust to your environment)

# module load your_app_dependencies             # Load any other required modules
module load cuda/12.1                         # Load CUDA toolkit
module load openmpi                           # Load MPI implementation
module load nvhpc/23.1


# Optional: Set environment variables for performance tuning
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK   # Set OpenMP threads per task
export NCCL_DEBUG=INFO                        # Enable NCCL debugging (for multi-GPU communication)

# Launch the distributed GPU application
# Replace with your actual command (e.g., mpirun or srun)
#srun --mpi=pmix ./my_distributed_gpu_app --config config.yaml
srun --mpi=none bash -c "./launch_scao.sh"

